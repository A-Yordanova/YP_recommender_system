Привет, Даниил! Спасибо за ревью! Дала комментарии по всем замечаниям. Где возникли встречные вопросы, я выделила их в тексте жирным шрифтом.

### Комментарий 1
В файле `airflow_service/1_prepare_dataset.py` ты оставил следующий комментарий:

```python
# Load Events
events = pd.read_sql("SELECT * FROM events", db_conn, index_col="id")
events_path = "/tmp/events.parquet"
events.to_parquet(events_path)
```

> Если используются динамические запросы, рекомендуется заранее фильтровать входные параметры или применять параметризованные запросы.

**Но здесь ведь не динамический запрос? Он ничего не принимает извне, events_path - это не параметр запроса, а путь, куда скинуть временный паркет-файл, который получился после выгрузки.
Поясни, пожалуйста, что ты имел в виду. Может, я не так поняла, что такое динамический запрос.**

### Комментарий 2
В файле `airflow_service/2_train_als_model.py` ты оставил следующий комментарий:
> Для больших таблиц выгрузка через Pandas может быть медленной. Можно использовать потоковую загрузку данных через copy_expert() в PostgreSQL.

**Не нашла примеров, как загрузить через copy_expert() сразу в parquet, только csv. Можешь подсказать, есть ли такая опция или только через танцы с бубнами? Вообще, это нормальная практика использовать csv для больших объемов данных?**

### Комментарий 3

Добавила во все DAGи логирование на каждом шаге. Логирование видно в интерфейсе Airflow, если зайти в запуск и перейти в Logs.
Добавила обработку возможных ошибок через try-except, и тоже с логированием.

### Комментарий 4

Я занесла в Variables Airflow такие переменные, как количество рекомендаций для разных типов, порог для персональных рекомендаций, количество дней для формирования тестовой выборки.
Теперь ничего не захардкожено, и может гибко изменяться при необходимости.

![настройки_airflow](/airflow_service/airflow_variables.jpg)

### Комментарий 5

Не поняла, как элегантно избавиться от цикла в `get_eligible_users()`. В курсе мне приходилось прибегать к услугам dask для распределенных вычислений, но я сохраняла временные файлы на каждые 10000 строк, делала точки загрузки с последнего упавшего батча, потом всё мучительно объединяла. Опыт получился так себе, не думаю, что это было оптимально. **Подскажи, пожалуйста, как правильно решить эту задачу.**

### Комментарий 6

Вынесла повторяющиеся функции `get_available_items`, `get_user_history`, `get_eligible_users` в отдельный файл `plugins/rec_functions.py`.
А функции `extract_tables`, `get_sqlalchemy_type`, `create_dynamic_table`, `insert_dataframe_to_table` добавила в `plugins/database_functions.py`.

### Комментарий 7

Добавила больше комментариев к коду, в больших функциях написала докстринги с описанием, что делают, какие параметры принимают на вход, что возвращают.